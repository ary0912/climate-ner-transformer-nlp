# ğŸ¤– AI Project â€” Climate Sentiment Classification & Twitter NER

![Typing Header](https://readme-typing-svg.demolab.com?font=Fira+Code&size=24&pause=1000&color=36D7B7&center=true&vCenter=true&width=850&lines=ğŸŒ¡ï¸+Classify+ESG+Disclosure+Sentiment;ğŸ¦+Extract+Entities+From+Tweet+Text)

<p align="center">
  <img src="https://img.shields.io/badge/NLP-Climate%20Sentiment-blue?style=for-the-badge"/>
  <img src="https://img.shields.io/badge/NER-Twitter-green?style=for-the-badge"/>
  <img src="https://img.shields.io/badge/Transformer-BERT/DistilBERT-yellow?style=for-the-badge"/>
</p>

---

## ğŸ§  Project Overview
This project integrates two natural language processing applications:

- **Classifying climate-related financial disclosures** into sentiment categories: Risk, Opportunity, or Neutral.
- **Performing Named Entity Recognition (NER)** on noisy Twitter text using transformer-based architectures.

All components are developed independently using transformer models and topic modeling techniques.

---

## ğŸŒ Climate Sentiment Classification

### ğŸ” Objective
Automatically identify whether a financial or ESG-related statement expresses Risk, Opportunity, or Neutral sentiment regarding climate impact.

### ğŸ“š Methods and Models
| Model       | Accuracy |
|-------------|----------|
| Naive Bayes | 0.79     |
| FNN         | 0.47     |
| BERT        | 0.66     |

- Naive Bayes: Tuned with `ngram_range=(2,2)` and `min_df=2`, outperforming contextual models on sparse data.
- Feedforward Neural Network: Utilized GloVe embeddings; underperformed due to lack of deep contextual signal.
- BERT: Fine-tuned with HuggingFace transformers; struggled with mixed sentiment tone but captured nuanced context.

### ğŸ“ˆ Feature Engineering
- Stopword filtering using sklearn  
- TF-IDF vectorization (with and without bigrams)  
- Max sequence length tuning for BERT: 128  
- Batch size: 16, epochs: 4  

---

## ğŸ“Š BERTopic Topic Modeling (Extension)

### âš™ï¸ Pipeline
- Sentence embedding: MiniLM via `sentence-transformers`
- Dimensionality reduction: UMAP (10 neighbors)
- Clustering: HDBSCAN
- Topic representation: c-TF-IDF

### ğŸ“Œ Results
Generated interpretable clusters:
- ğŸ§± Risk: climate, disaster, impact, liability  
- ğŸŒ Opportunity: energy, transition, investment  
- ğŸš— Green Tech: EV, carbon, hydrogen  

BERTopic achieved better topic coherence than LDA, particularly on short ESG documents.

---

## ğŸ¦ Twitter Named Entity Recognition (NER)

### ğŸ” Goal
Identify named entities (Person, Location, Corporation, Product, etc.) in informal and noisy Twitter data.

### ğŸ¤– Model
- Pretrained model: `distilbert-base-uncased`
- Fine-tuned using HuggingFace Trainer
- Dataset: Broad Twitter Corpus (BTC)

### ğŸ”§ Token Alignment
- WordPiece tokenization handled via token-level label alignment
- Subword masking applied to ignore non-initial subwords during training

### ğŸ“Š Metrics
| Entity Type     | Precision | Recall | F1    |
|------------------|-----------|--------|-------|
| Person           | 0.73      | 0.68   | 0.70  |
| Location         | 0.69      | 0.63   | 0.66  |
| Corporation      | 0.65      | 0.58   | 0.61  |
| Product          | 0.50      | 0.56   | 0.53  |

Final macro-averaged F1 score: **0.60**

---

## ğŸ§° Technologies Used
- Python 3.10+
- `scikit-learn`, `transformers`, `pandas`, `nltk`, `matplotlib`, `seaborn`
- `BERTopic`, `MiniLM`, `UMAP`, `HDBSCAN`, `sentence-transformers`
- `seqeval`, `HuggingFace Trainer`, `c-TFIDF`

---

## ğŸš€ Getting Started

### ğŸ“¦ Install Dependencies
```bash
pip install -r requirements.txt
```
<p align="center"> <img src="https://readme-typing-svg.demolab.com?font=Fira+Code&size=20&pause=2000&color=F97316&center=true&vCenter=true&width=900&lines=From+climate+disclosures+to+tweet+NER...;Transformers+decoded+semantic+insight+across+domains."/> </p>
